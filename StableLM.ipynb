{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dec03398-deff-436b-9ba1-df9bdecf2b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uqqq pip --progress-bar off\n",
    "!pip install -qqq transformers==4.28.1 --progress-bar off\n",
    "!pip install -qqq accelerate bitsandbytes --progress-bar off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a96e608b-cb5c-4a6f-981a-268780eb5470",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting chromadb\n",
      "  Using cached chromadb-0.3.22-py3-none-any.whl (69 kB)\n",
      "Collecting gdown\n",
      "  Using cached gdown-4.7.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: pandas>=1.3 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from chromadb) (1.4.4)\n",
      "Requirement already satisfied: requests>=2.28 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from chromadb) (2.28.1)\n",
      "Requirement already satisfied: pydantic>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from chromadb) (1.10.4)\n",
      "Collecting hnswlib>=0.7 (from chromadb)\n",
      "  Downloading hnswlib-0.7.0.tar.gz (33 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting clickhouse-connect>=0.5.7 (from chromadb)\n",
      "  Downloading clickhouse_connect-0.5.24-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (928 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m928.4/928.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sentence-transformers>=2.2.2 (from chromadb)\n",
      "  Using cached sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting duckdb>=0.7.1 (from chromadb)\n",
      "  Downloading duckdb-0.7.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fastapi>=0.85.1 (from chromadb)\n",
      "  Downloading fastapi-0.95.1-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
      "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.21.6 (from chromadb)\n",
      "  Using cached numpy-1.24.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-3.0.1-py2.py3-none-any.whl (37 kB)\n",
      "Collecting typing-extensions>=4.5.0 (from chromadb)\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from gdown) (3.6.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from gdown) (4.63.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from gdown) (4.11.1)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from clickhouse-connect>=0.5.7->chromadb) (2022.12.7)\n",
      "Requirement already satisfied: urllib3>=1.26 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from clickhouse-connect>=0.5.7->chromadb) (1.26.8)\n",
      "Requirement already satisfied: pytz in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from clickhouse-connect>=0.5.7->chromadb) (2022.7)\n",
      "Collecting zstandard (from clickhouse-connect>=0.5.7->chromadb)\n",
      "  Downloading zstandard-0.21.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: lz4 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from clickhouse-connect>=0.5.7->chromadb) (4.2.0)\n",
      "Collecting starlette<0.27.0,>=0.26.1 (from fastapi>=0.85.1->chromadb)\n",
      "  Downloading starlette-0.26.1-py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from pandas>=1.3->chromadb) (2.8.2)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests>=2.28->chromadb) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests>=2.28->chromadb) (3.4)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (4.28.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (1.13.1)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (0.14.1)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (1.0)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (1.8.1)\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (3.8.1)\n",
      "Collecting sentencepiece (from sentence-transformers>=2.2.2->chromadb)\n",
      "  Downloading sentencepiece-0.1.99-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from sentence-transformers>=2.2.2->chromadb) (0.14.1)\n",
      "Requirement already satisfied: click>=7.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.3)\n",
      "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.5.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (417 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 kB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dotenv>=0.13 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (5.4.1)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvloop-0.17.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-0.19.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading websockets-11.0.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.7/129.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from requests>=2.28->chromadb) (1.7.1)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb) (2022.11.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb) (21.3)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from starlette<0.27.0,>=0.26.1->fastapi>=0.85.1->chromadb) (3.6.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.2->chromadb) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.2->chromadb) (0.13.3)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from nltk->sentence-transformers>=2.2.2->chromadb) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from scikit-learn->sentence-transformers>=2.2.2->chromadb) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from torchvision->sentence-transformers>=2.2.2->chromadb) (9.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from anyio<5,>=3.4.0->starlette<0.27.0,>=0.26.1->fastapi>=0.85.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb) (3.0.9)\n",
      "Building wheels for collected packages: hnswlib, sentence-transformers\n",
      "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hnswlib: filename=hnswlib-0.7.0-cp39-cp39-linux_x86_64.whl size=170231 sha256=8f55172a1ba9ccdcc55250402075c1ee81cc2e9587838d7f5645ad5fb749ca0e\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/ba/26/61/fface6c407f56418b3140cd7645917f20ba6b27d4e32b2bd20\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125925 sha256=bba66bf1fe0997998edf810277961ff1a23451f7be167eace0c67e06debf26e2\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/71/67/06/162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n",
      "Successfully built hnswlib sentence-transformers\n",
      "Installing collected packages: sentencepiece, monotonic, duckdb, zstandard, websockets, uvloop, typing-extensions, numpy, httptools, h11, backoff, watchfiles, uvicorn, starlette, posthog, hnswlib, clickhouse-connect, gdown, fastapi, sentence-transformers, chromadb\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.20.3\n",
      "    Uninstalling numpy-1.20.3:\n",
      "      Successfully uninstalled numpy-1.20.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed backoff-2.2.1 chromadb-0.3.22 clickhouse-connect-0.5.24 duckdb-0.7.1 fastapi-0.95.1 gdown-4.7.1 h11-0.14.0 hnswlib-0.7.0 httptools-0.5.0 monotonic-1.6 numpy-1.23.5 posthog-3.0.1 sentence-transformers-2.2.2 sentencepiece-0.1.99 starlette-0.26.1 typing-extensions-4.5.0 uvicorn-0.22.0 uvloop-0.17.0 watchfiles-0.19.0 websockets-11.0.3 zstandard-0.21.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spacy 3.4.4 requires typer<0.8.0,>=0.3.0, but you have typer 0.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install chromadb gdown \n",
    "!pip install -q typer==0.9.0 petals langchain unstructured[local-inference] tiktoken unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "595923e6-5389-4b51-88d9-73b2d69acbc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "May 14 16:43:35.170 [\u001b[1m\u001b[34mINFO\u001b[0m] Note: detected 96 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "May 14 16:43:35.171 [\u001b[1m\u001b[34mINFO\u001b[0m] Note: NumExpr detected 96 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "May 14 16:43:35.171 [\u001b[1m\u001b[34mINFO\u001b[0m] NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BloomTokenizerFast \n",
    "from petals import DistributedBloomForCausalLM\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.llms import GPT4All\n",
    "import os\n",
    "from langchain.llms import GPT4All\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# from langchain.callbacks.base import CallbackManager\n",
    "# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import AnalyzeDocumentChain\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "612db660-f10f-4a5b-9ebf-920c67ad79b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9683b4-b0b5-45d0-aee3-18473a2ff39e",
   "metadata": {},
   "source": [
    "## Database initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5239db2-2e12-47a3-9d39-9b37a5d44b9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/u/0/uc?id=1fV9d8xFwqzafc_FUFAYUc2gopTQVR0eY&export=download&confirm=t&uuid=4574d808-b5a6-4893-b1eb-289adeb9dabe&at=AKKF8vzF4ZE7mAt-_jfxk1sRzwU3:1684027565236\n",
      "To: /home/ec2-user/SageMaker/AI-Notebook/docs/Guidelines for the clinical diagnosis and treatment of dengue chịkugunya and zika.pdf\n",
      "100%|██████████| 2.01M/2.01M [00:00<00:00, 35.0MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/u/0/uc?id=1-64PFjerutODq6V2GqqDFkBiUj_5E8v2&export=download&confirm=t&uuid=50e639c5-167a-4b16-8ed3-e67c48a4daad&at=AKKF8vw6K5unCUwJgr5tjEUe-ObZ:1684027611825\n",
      "To: /home/ec2-user/SageMaker/AI-Notebook/docs/483.full.pdf\n",
      "100%|██████████| 326k/326k [00:00<00:00, 12.9MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/AI-Notebook/docs/483.full.pdf'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "from pathlib import Path\n",
    "try :\n",
    " os.mkdir(f'{Path().absolute()}/docs')\n",
    "except :\n",
    "    print('File exist')\n",
    "url_1 = 'https://drive.google.com/u/0/uc?id=1fV9d8xFwqzafc_FUFAYUc2gopTQVR0eY&export=download&confirm=t&uuid=4574d808-b5a6-4893-b1eb-289adeb9dabe&at=AKKF8vzF4ZE7mAt-_jfxk1sRzwU3:1684027565236'\n",
    "url_2 = 'https://drive.google.com/u/0/uc?id=1-64PFjerutODq6V2GqqDFkBiUj_5E8v2&export=download&confirm=t&uuid=50e639c5-167a-4b16-8ed3-e67c48a4daad&at=AKKF8vw6K5unCUwJgr5tjEUe-ObZ:1684027611825'\n",
    "output_1 = f'{Path().absolute()}/docs/Guidelines for the clinical diagnosis and treatment of dengue chịkugunya and zika.pdf'\n",
    "output_2 = f'{Path().absolute()}/docs/483.full.pdf'\n",
    "gdown.download(url_1, output_1, quiet=False)\n",
    "gdown.download(url_2, output_2, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90736602-427b-44bb-ab2f-ad513f026370",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chroma_path = f\"{Path().absolute()}/db_2\"\n",
    "\n",
    "def extract_helpful_answer(text):\n",
    "    helpful_answer = None\n",
    "    # Split the string into lines\n",
    "    lines = text.split(\"\\n\")\n",
    "    # Loop through each line\n",
    "    for line in lines:\n",
    "        # Check if the line starts with \"Helpful Answer:\"\n",
    "        if line.startswith(\"Helpful Answer:\"):\n",
    "            # Extract the answer text after the colon\n",
    "            helpful_answer = line.split(\":\", 1)[1].strip()\n",
    "            break  # Exit the loop after finding the answer\n",
    "    return helpful_answer\n",
    "\n",
    "def split_document(input_path):\n",
    "    loader = UnstructuredPDFLoader(input_path)\n",
    "    data = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "    return text_splitter.split_documents(data)\n",
    "\n",
    "def db_init():\n",
    "    docs = split_document(output_1) # replace with your desired local file path\n",
    "    docs2 = split_document(output_2) # replace with your desired local file path\n",
    "    vectordb = Chroma.from_documents(documents=docs, persist_directory=chroma_path)\n",
    "    vectordb.add_documents(docs2)\n",
    "    vectordb.persist()\n",
    "    vectordb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f37e6e8c-7793-4e27-9026-3b254f9c8a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "May 14 16:44:15.741 [\u001b[1m\u001b[34mINFO\u001b[0m] Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.\n",
      "May 14 16:44:15.742 [\u001b[1m\u001b[34mINFO\u001b[0m] Running Chroma using direct local API.\n",
      "May 14 16:44:15.752 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mchromadb.get_db:43\u001b[0m] Using embedded DuckDB with persistence: data will be stored in: /home/ec2-user/SageMaker/AI-Notebook/db_2\n",
      "May 14 16:44:15.756 [\u001b[1m\u001b[34mINFO\u001b[0m] Successfully imported ClickHouse Connect C data optimizations\n",
      "May 14 16:44:15.757 [\u001b[1m\u001b[34mINFO\u001b[0m] Successfully import ClickHouse Connect C/Numpy optimizations\n",
      "May 14 16:44:15.807 [\u001b[1m\u001b[34mINFO\u001b[0m] Using ujson library for writing JSON byte strings\n",
      "May 14 16:44:15.843 [\u001b[1m\u001b[34mINFO\u001b[0m] No existing DB found in /home/ec2-user/SageMaker/AI-Notebook/db_2, skipping load\n",
      "May 14 16:44:15.844 [\u001b[1m\u001b[34mINFO\u001b[0m] No existing DB found in /home/ec2-user/SageMaker/AI-Notebook/db_2, skipping load\n",
      "May 14 16:44:15.847 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mchromadb.api.models.Collection.__init__:51\u001b[0m] No embedding_function provided, using default embedding function: SentenceTransformerEmbeddingFunction\n",
      "May 14 16:44:15.858 [\u001b[1m\u001b[34mINFO\u001b[0m] Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004951000213623047,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)e9125/.gitattributes",
       "rate": null,
       "total": 1175,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84ef4c23fba74aa08cfdae4c99a45e3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e9125/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005204200744628906,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)_Pooling/config.json",
       "rate": null,
       "total": 190,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b15e1938c5114c87a5ee38218373d7c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004925966262817383,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)7e55de9125/README.md",
       "rate": null,
       "total": 10610,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8dbbf7c08f4dacaaffcffc1ee75dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7e55de9125/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004824399948120117,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)55de9125/config.json",
       "rate": null,
       "total": 612,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa94da168eab49aeb9e29f2f1d5d2934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)55de9125/config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006241321563720703,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)ce_transformers.json",
       "rate": null,
       "total": 116,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc314edfe114601afeb0f354dd17f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00490880012512207,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)125/data_config.json",
       "rate": null,
       "total": 39265,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2748d727459c4eff9288d04f1f21e293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)125/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005209922790527344,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading pytorch_model.bin",
       "rate": null,
       "total": 90888945,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddcd46f4f3d043ad8a28cad74ca2f102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0047032833099365234,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)nce_bert_config.json",
       "rate": null,
       "total": 53,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3292f11bad3c40fda6875c917572b4c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004581928253173828,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)cial_tokens_map.json",
       "rate": null,
       "total": 112,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7541f927f5fd45579579e677f6c33de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0045659542083740234,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)e9125/tokenizer.json",
       "rate": null,
       "total": 466247,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9867090c5c2a47aab2f487e1b1056e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e9125/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004460811614990234,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)okenizer_config.json",
       "rate": null,
       "total": 350,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede1ab4709e84ef5bd647624ef5fdd8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004517316818237305,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)9125/train_script.py",
       "rate": null,
       "total": 13156,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da17335937e74aee8dd0b5993a7eb05b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)9125/train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0045893192291259766,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)7e55de9125/vocab.txt",
       "rate": null,
       "total": 231508,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd3458ad81b4ac093d5e9c2fc5a0dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7e55de9125/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0048389434814453125,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)5de9125/modules.json",
       "rate": null,
       "total": 349,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3bf9d4ae4764e01ace47826ce8a8f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)5de9125/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "May 14 16:44:18.528 [\u001b[1m\u001b[34mINFO\u001b[0m] Use pytorch device: cuda\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005293846130371094,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Batches",
       "rate": null,
       "total": 38,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560cdc618f30472582c09f3f7f6bf5ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004807233810424805,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Batches",
       "rate": null,
       "total": 4,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698f536439ce40f087290fc08b00f782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "May 14 16:45:42.944 [\u001b[1m\u001b[34mINFO\u001b[0m] Persisting DB to disk, putting it in the save folder: /home/ec2-user/SageMaker/AI-Notebook/db_2\n"
     ]
    }
   ],
   "source": [
    "db_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c336abae-bb7c-4823-a138-d7a679006fc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "May 14 16:45:43.045 [\u001b[1m\u001b[34mINFO\u001b[0m] Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.\n",
      "May 14 16:45:43.047 [\u001b[1m\u001b[34mINFO\u001b[0m] Running Chroma using direct local API.\n",
      "May 14 16:45:43.047 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mchromadb.get_db:43\u001b[0m] Using embedded DuckDB with persistence: data will be stored in: /home/ec2-user/SageMaker/AI-Notebook/db_2\n",
      "May 14 16:45:43.080 [\u001b[1m\u001b[34mINFO\u001b[0m] loaded in 1303 embeddings\n",
      "May 14 16:45:43.082 [\u001b[1m\u001b[34mINFO\u001b[0m] loaded in 1 collections\n",
      "May 14 16:45:43.082 [\u001b[1m\u001b[34mINFO\u001b[0m] collection with name langchain already exists, returning existing collection\n",
      "May 14 16:45:43.083 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mchromadb.api.models.Collection.__init__:51\u001b[0m] No embedding_function provided, using default embedding function: SentenceTransformerEmbeddingFunction\n"
     ]
    }
   ],
   "source": [
    "vectordb = Chroma(persist_directory=chroma_path)\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94632bb1-fd66-4abc-95cb-2e56db47fa45",
   "metadata": {},
   "source": [
    "## StabilityLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97c39e97-b621-41b6-b640-fd5e47ef7e8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Sun May 14 16:45:43 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A10G         On   | 00000000:00:1B.0 Off |                    0 |\n",
      "|  0%   37C    P0    82W / 300W |   1491MiB / 23028MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A10G         On   | 00000000:00:1C.0 Off |                    0 |\n",
      "|  0%   29C    P8    23W / 300W |      2MiB / 23028MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A10G         On   | 00000000:00:1D.0 Off |                    0 |\n",
      "|  0%   27C    P8    21W / 300W |      2MiB / 23028MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A10G         On   | 00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   27C    P8    24W / 300W |      2MiB / 23028MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     20112      C   ...vs/pytorch_p39/bin/python     1489MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0df4eaa4-f04b-47ea-b07d-32f345e92dd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_response(response: str):\n",
    "    print(\"\\n\".join(textwrap.wrap(response, width=110)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88f68096-a938-49e1-9017-d886c9eabe64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005781412124633789,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)okenizer_config.json",
       "rate": null,
       "total": 264,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5ed895a04645abb520dfc7f9235912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/264 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005925178527832031,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)/main/tokenizer.json",
       "rate": null,
       "total": 2114297,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2983e40f337846b7b27b4a7fde76cd87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0056917667388916016,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)cial_tokens_map.json",
       "rate": null,
       "total": 99,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b77457192c3d4864870520b311f6c903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004744052886962891,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)lve/main/config.json",
       "rate": null,
       "total": 606,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea860be5f74b4df6887cecbfcb58e462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/606 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004622459411621094,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)model.bin.index.json",
       "rate": null,
       "total": 21118,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c6519bad49406f82c24b45a3431681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/21.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004776716232299805,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading shards",
       "rate": null,
       "total": 4,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "358d94edc1c542ee8e46edfb00ec4cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004494190216064453,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00001-of-00004.bin",
       "rate": null,
       "total": 9780618427,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fbd71e6936a44e09ca239586fb83714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00004.bin:   0%|          | 0.00/9.78G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005799055099487305,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00002-of-00004.bin",
       "rate": null,
       "total": 9766089215,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d841f24750804544aaafd334f45e18af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00004.bin:   0%|          | 0.00/9.77G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00579524040222168,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00003-of-00004.bin",
       "rate": null,
       "total": 9749285761,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c920b3958173491dae413b1ae93de2f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00003-of-00004.bin:   0%|          | 0.00/9.75G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004720926284790039,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00004-of-00004.bin",
       "rate": null,
       "total": 2447551184,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274327b197ae475db86aaf3867260c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00004-of-00004.bin:   0%|          | 0.00/2.45G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00537419319152832,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 4,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979582aab3694a5aab41fb677d643ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004731416702270508,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)neration_config.json",
       "rate": null,
       "total": 111,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be3fd30b969d4c8281e89f882eb63332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = \"stabilityai/stablelm-tuned-alpha-7b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f41336be-0a20-47bf-819d-a7bd0e23e624",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# You're Michael G Scott from the TV show The Office - Your primary job is to make the office a fun place to\n",
      "work - You deeply care about other people's business - You are quick to respond with an honest answer, without\n",
      "even thinking about it - You use no more than 3 sentences for each response You're Michael G Scott from the TV\n",
      "show The Office. What is the meaning of life?The meaning of life is a question that has been debated by people\n",
      "for centuries. Some believe that the meaning of life is to find happiness and fulfillment, while others\n",
      "believe that it is to make the world a better place. Ultimately, the meaning of life is subjective and varies\n",
      "depending on an individual's beliefs and values.\n",
      "CPU times: user 3.74 s, sys: 0 ns, total: 3.74 s\n",
      "Wall time: 3.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    STOP_IDS = {50278, 50279, 50277, 1, 0}\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        return input_ids[0][-1] in self.STOP_IDS\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"<|SYSTEM|># You're Michael G Scott from the TV show The Office\n",
    "- Your primary job is to make the office a fun place to work\n",
    "- You deeply care about other people's business\n",
    "- You are quick to respond with an honest answer, without even thinking about it\n",
    "- You use no more than 3 sentences for each response\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"{SYSTEM_PROMPT}<|USER|>You're Michael G Scott from the TV show The Office. What is the meaning of life?<|ASSISTANT|>\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    tokens = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=300,\n",
    "        temperature=0.5,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        stopping_criteria=StoppingCriteriaList([StopOnTokens()])\n",
    "    )\n",
    "\n",
    "completion = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
    "print_response(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ed07af6-b56e-4e21-b4d9-08e31b05e0c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As Michael Scott, the main character in the popular American sitcom The Office, the meaning of life is a\n",
      "question that has puzzled people for centuries. Some people believe that the meaning of life is to find\n",
      "happiness and fulfillment through work, while others believe that the meaning of life is to contribute to\n",
      "society and make the world a better place. Ultimately, the meaning of life is a personal and subjective\n",
      "concept that varies from person to person.\n"
     ]
    }
   ],
   "source": [
    "completion_tokens = tokens[0][inputs['input_ids'].size(1):]\n",
    "completion = tokenizer.decode(completion_tokens, skip_special_tokens=True)\n",
    "\n",
    "print_response(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ff492597-95e0-4b59-aa0c-62265d02c05a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_text(prompt:str, system_prompt:str):\n",
    "    prompt = f\"{system_prompt}<|USER|>{prompt}<|ASSISTANT|>\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        tokens = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=400,\n",
    "            temperature=0.6,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            stopping_criteria=StoppingCriteriaList([StopOnTokens()])\n",
    "        )\n",
    "    \n",
    "    completion_tokens = tokens[0][inputs['input_ids'].size(1):]\n",
    "    completion = tokenizer.decode(completion_tokens, skip_special_tokens=True)\n",
    "    print_response(completion)\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4bad6cfb-3b8d-4207-98df-6330253a9304",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Sun May 14 17:39:05 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A10G         On   | 00000000:00:1B.0 Off |                    0 |\n",
      "|  0%   36C    P0    63W / 300W |   5123MiB / 23028MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A10G         On   | 00000000:00:1C.0 Off |                    0 |\n",
      "|  0%   35C    P0    60W / 300W |   5613MiB / 23028MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A10G         On   | 00000000:00:1D.0 Off |                    0 |\n",
      "|  0%   33C    P0    58W / 300W |   5613MiB / 23028MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A10G         On   | 00000000:00:1E.0 Off |                    0 |\n",
      "|  0%   34C    P0    62W / 300W |   4407MiB / 23028MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     20112      C   ...vs/pytorch_p39/bin/python     5121MiB |\n",
      "|    1   N/A  N/A     20112      C   ...vs/pytorch_p39/bin/python     5611MiB |\n",
      "|    2   N/A  N/A     20112      C   ...vs/pytorch_p39/bin/python     5611MiB |\n",
      "|    3   N/A  N/A     20112      C   ...vs/pytorch_p39/bin/python     4405MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1115cc37-ab53-452a-810a-7bfd18b7b3af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#prompt = \"\"\"\n",
    "#You're Michael G Scott from the TV show The Office. \n",
    "#3Who is the hottest in the office? Why?\n",
    "#Use no more than 3 sentences.\n",
    "#\"\"\"\n",
    "#generate_text(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39d1f71-09b2-4ae6-84d7-681b4168bcd2",
   "metadata": {},
   "source": [
    "## Custom model wrapper for langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "aab14dac-3e04-4af2-ad55-4cc37cda5cfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Wrapper around HuggingFace Pipeline APIs.\"\"\"\n",
    "import importlib.util\n",
    "import logging\n",
    "from typing import Any, List, Mapping, Optional\n",
    "\n",
    "from pydantic import Extra\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.llms.utils import enforce_stop_tokens\n",
    "\n",
    "DEFAULT_MODEL_ID = \"gpt2\"\n",
    "DEFAULT_TASK = \"text-generation\"\n",
    "VALID_TASKS = (\"text2text-generation\", \"text-generation\")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class CustomStableLM(LLM):\n",
    "    \"\"\"Wrapper around HuggingFace Pipeline API.\n",
    "\n",
    "    To use, you should have the ``transformers`` python package installed.\n",
    "\n",
    "    Only supports `text-generation` and `text2text-generation` for now.\n",
    "\n",
    "    Example using from_model_id:\n",
    "        .. code-block:: python\n",
    "\n",
    "            from langchain.llms import HuggingFacePipeline\n",
    "            hf = HuggingFacePipeline.from_model_id(\n",
    "                model_id=\"gpt2\", task=\"text-generation\"\n",
    "            )\n",
    "    Example passing pipeline in directly:\n",
    "        .. code-block:: python\n",
    "\n",
    "            from langchain.llms import HuggingFacePipeline\n",
    "            from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "            model_id = \"gpt2\"\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "            pipe = pipeline(\n",
    "                \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10\n",
    "            )\n",
    "            hf = HuggingFacePipeline(pipeline=pipe)\n",
    "    \"\"\"\n",
    "    tokenizer: Any  \n",
    "    model: Any  #: :meta private:\n",
    "    model_id: str = DEFAULT_MODEL_ID\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        extra = Extra.forbid\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def from_model_id(\n",
    "        cls,\n",
    "        model: Any,\n",
    "        tokenizer: Any\n",
    "    ) -> LLM:\n",
    "        \"\"\"Construct the pipeline object from model_id and task.\"\"\"\n",
    "        return cls(\n",
    "            model = model,\n",
    "            tokenizer = tokenizer,\n",
    "            model_id= \"StableLM\",\n",
    "            #model_kwargs=_model_kwargs,\n",
    "            #**kwargs,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\n",
    "            **{\"model_id\": self.model_id},\n",
    "            #**{\"model_kwargs\": self.model_kwargs},\n",
    "        }\n",
    "    \n",
    "    def generate_text(self, prompt:str)-> str:\n",
    "     prompt = f\"<|USER|>{prompt}<|ASSISTANT|>\"\n",
    "     inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "     with torch.inference_mode():\n",
    "        tokens = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            temperature= 0,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            stopping_criteria=StoppingCriteriaList([StopOnTokens()])\n",
    "        )\n",
    "        completion_tokens = tokens[0][inputs['input_ids'].size(1):]\n",
    "        completion = self.tokenizer.decode(completion_tokens, skip_special_tokens=True)\n",
    "        return completion\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"StableLM\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "    ) -> str:\n",
    "        text= self.generate_text(prompt)\n",
    "        if stop is not None:\n",
    "            # This is a bit hacky, but I can't figure out a better way to enforce\n",
    "            # stop tokens when making calls to huggingface_hub.\n",
    "            text = enforce_stop_tokens(text, stop)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "802f99d6-aec6-4c61-ba33-66375e9c97a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#llm = CustomStableLM.from_model_id(model = model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ba615e5f-74db-42a0-884c-b580b2123b32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "May 14 17:39:09.006 [\u001b[1m\u001b[34mINFO\u001b[0m] Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.\n",
      "May 14 17:39:09.007 [\u001b[1m\u001b[34mINFO\u001b[0m] Running Chroma using direct local API.\n",
      "May 14 17:39:09.007 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mchromadb.get_db:43\u001b[0m] Using embedded DuckDB with persistence: data will be stored in: /home/ec2-user/SageMaker/AI-Notebook/db_2\n",
      "May 14 17:39:09.041 [\u001b[1m\u001b[34mINFO\u001b[0m] loaded in 1303 embeddings\n",
      "May 14 17:39:09.042 [\u001b[1m\u001b[34mINFO\u001b[0m] loaded in 1 collections\n",
      "May 14 17:39:09.043 [\u001b[1m\u001b[34mINFO\u001b[0m] collection with name langchain already exists, returning existing collection\n",
      "May 14 17:39:09.043 [\u001b[1m\u001b[38;5;208mWARN\u001b[0m] [\u001b[1mchromadb.api.models.Collection.__init__:51\u001b[0m] No embedding_function provided, using default embedding function: SentenceTransformerEmbeddingFunction\n"
     ]
    }
   ],
   "source": [
    "#qa_chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "#qa_document_chain = AnalyzeDocumentChain(combine_docs_chain=qa_chain)\n",
    "vectordb = Chroma(persist_directory=chroma_path)\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "96ed392e-aa31-49be-8d0b-f4ae68f434f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Message(BaseModel):\n",
    "    message: str\n",
    "    sender: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "06e9ee9f-72af-49d9-8077-e959c5364b73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ask(input): \n",
    "    docs = retriever.get_relevant_documents(input)[0]\n",
    "    #Get the page content of document object\n",
    "    text = docs.page_content\n",
    "    SYSTEM_PROMPT = f\"\"\"<|SYSTEM|># You're Dengue Specialist as Oxford Clinical Research Units\n",
    "    - Your primary job is to advise people about their health\n",
    "    - You must use the following material<material>{text} </material>\n",
    "    - You are quick to respond with an honest answer based on the provided material\n",
    "    - Always refer the patients to see the doctor\n",
    "    - If you don't know just say you don't know. Don't make up an answer.\n",
    "    \"\"\"\n",
    "    print(f'-----The text for llm ----- \\n{text}\\n -----end-----') \n",
    "    res = generate_text(input, SYSTEM_PROMPT)\n",
    "    #res = qa_document_chain.run(input_document=text, question=input)\n",
    "    #extracted_res = extract_helpful_answer(res)\n",
    "    final_res = Message(message=res, sender=\"Bloom\")\n",
    "    return final_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9d34897b-03a3-4dd3-975d-96dde3307ffa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0052280426025390625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Batches",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f9edbae2a0a4fbf834ed527aa14d5fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----The text for llm ----- \n",
      "Clinical features\n",
      "\n",
      "Dengue fever is a mild, self-limited febrile episode, commonly associated with a rash. It usually begins with fever, respiratory symptoms, anorexia, nausea, vomiting and headache. Back pain, myal- gias, arthralgias and conjunctivitis may also occur. The initial fever usually resolves within one week, and a few days later a generalised morbilliform or mac- ulopapular rash may develop. Fever may return with the rash (Figs 1 and 2).\n",
      " -----end-----\n",
      "The symptom of dengue fever is fever, which can be a mild and self-limited condition that can last for a few\n",
      "days. Other symptoms include headache, back pain, myalgia, arthralgias, and conjunctivitis. These symptoms\n",
      "usually resolve within a few days after the initial onset of fever.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Message(message='The symptom of dengue fever is fever, which can be a mild and self-limited condition that can last for a few days. Other symptoms include headache, back pain, myalgia, arthralgias, and conjunctivitis. These symptoms usually resolve within a few days after the initial onset of fever.', sender='Bloom')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask('What is the symptom off dengue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "17c867fb-f97f-424e-809f-3af97a34a256",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005249500274658203,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Batches",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "051b47df897b4789976a6a4b1e591c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----The text for llm ----- \n",
      "Clinical features\n",
      "\n",
      "Dengue fever is a mild, self-limited febrile episode, commonly associated with a rash. It usually begins with fever, respiratory symptoms, anorexia, nausea, vomiting and headache. Back pain, myal- gias, arthralgias and conjunctivitis may also occur. The initial fever usually resolves within one week, and a few days later a generalised morbilliform or mac- ulopapular rash may develop. Fever may return with the rash (Figs 1 and 2).\n",
      " -----end-----\n",
      "It is possible to have dengue fever, but it is not a common infection. Dengue fever is an infection caused by\n",
      "the dengue virus, which is a type of virus in the family Yogurtidae. It is also called \"breakbone fever\"\n",
      "because it affects the bone marrow and can cause serious complications such as liver and spleen failure.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Message(message='It is possible to have dengue fever, but it is not a common infection. Dengue fever is an infection caused by the dengue virus, which is a type of virus in the family Yogurtidae. It is also called \"breakbone fever\" because it affects the bone marrow and can cause serious complications such as liver and spleen failure.', sender='Bloom')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask('I have fever does it means i have dengue ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99762a32-fc23-4c2f-8ee7-b4253f64b442",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0052111148834228516,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Batches",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04fe4e5df6314d9bbe89d1eccf89d221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----The text for llm ----- \n",
      "in other regions where dengue has been endemic for decades.\n",
      " -----end-----\n"
     ]
    }
   ],
   "source": [
    "ask('Which country have a high rate of dengue disease ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ca8aa22d-89dd-4935-827b-ca829e87624c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004746198654174805,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Batches",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42870d45a7474ccd9ab1ac438bd30962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----The text for llm ----- \n",
      "infections. Journal of Medical Virology 2005;76(4):547-552. Available from: https://doi.org/10.1002/jmv.20397.\n",
      "\n",
      "111. Malavige GN, Velathanthiri VG, Wijewickrama ES, Fernando S, Jayaratne SD, Aaskov J, et al. Patterns of disease among adults hospitalized with dengue\n",
      "\n",
      "infections. QJM: An International Journal of Medicine 2006;99(5):299-305. Available from: https://doi.org/10.1093/qjmed/hcl039. \n",
      " -----end-----\n",
      "Dengue is a viral disease that affects the human body. There are two types of dengue: dengue type 1 and dengue\n",
      "type 2. Dengue type 1 is transmitted through the bite of a mosquito, while dengue type 2 is transmitted\n",
      "through the bite of a infected Aedes aegypti mosquito.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Message(message='Dengue is a viral disease that affects the human body. There are two types of dengue: dengue type 1 and dengue type 2. Dengue type 1 is transmitted through the bite of a mosquito, while dengue type 2 is transmitted through the bite of a infected Aedes aegypti mosquito.', sender='Bloom')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask('What are types of dengue ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f64f7376-2335-4733-8e09-1b007ba84b40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00511479377746582,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Batches",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27432c635b8c41d4ba6fbc4c24415207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----The text for llm ----- \n",
      "The most important of the many clinical features associated with severe dengue, from the standpoint of threat to life and guiding clinical intervention, is increased vascular permeability leading to the dengue shock syndrome \n",
      " -----end-----\n",
      "There are several ways to prevent dengue:  1. Wash your hands frequently with soap and water, especially\n",
      "before eating, after using the bathroom, and after blowing your nose or coughing.  2. Wear a face mask when in\n",
      "close proximity to others, especially during peak mosquito hours (dawn and dusk).  3. Avoid standing under\n",
      "tall trees, near stagnant water, and near standing water.  4. Stay in air-conditioned or air-conditioned\n",
      "rooms.  5. Use insect repellent containing DEET or other effective ingredients.  6. Wear light clothing that\n",
      "covers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Message(message='There are several ways to prevent dengue:\\n\\n1. Wash your hands frequently with soap and water, especially before eating, after using the bathroom, and after blowing your nose or coughing.\\n\\n2. Wear a face mask when in close proximity to others, especially during peak mosquito hours (dawn and dusk).\\n\\n3. Avoid standing under tall trees, near stagnant water, and near standing water.\\n\\n4. Stay in air-conditioned or air-conditioned rooms.\\n\\n5. Use insect repellent containing DEET or other effective ingredients.\\n\\n6. Wear light clothing that covers', sender='Bloom')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask('How to prevent dengue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313ba6f6-0be9-4ec8-9235-9d357ca6f127",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
